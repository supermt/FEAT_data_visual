Initializing RocksDB Options from the specified file
Initializing RocksDB Options from command-line flags
/home/supermt/rocksdb_hdd
Keys:       10 bytes each
Values:     1000 bytes each (500 bytes after compression)
Entries:    48318382
Prefix:    0 bytes
Keys per prefix:    0
RawSize:    46540.8 MB (estimated)
FileSize:   23500.8 MB (estimated)
Write rate: 0 bytes/second
Read rate: 0 ops/second
Compression: NoCompression
Compression sampling rate: 0
Memtablerep: skip_list
Perf Level: 1
------------------------------------------------
0 point(s) detected
use the FEAT tuner 
Initializing RocksDB Options from the specified file
Initializing RocksDB Options from command-line flags
/home/supermt/rocksdb_hdd
DB path: [/home/supermt/rocksdb_hdd]
using reporter agent with change points.
Using FEAT tuner.
 FEA is triggered
TEA is triggered
insert start:0
record count:60000000
op count:60000000
new keys:6000000
slow flush, decrease thread
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flush, decrease thread
ro increase, thread
ro, increase batch
lo, increase batch
slow flush, decrease thread
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flushing, increase batch
slow flush, decrease thread
ycsb_load    :      28.775 micros/op 34751 ops/sec;   33.9 MB/s
Microseconds per write:
Count: 60000000 Average: 28.7754  StdDev: 50.62
Min: 2  Median: 3.5172  Max: 56352267
Percentiles: P50: 3.52 P75: 4.72 P99: 35.29 P99.9: 1226.57 P99.99: 1293.27
------------------------------------------------------
(       1,       2 ]   971316   1.619%   1.619% 
(       2,       3 ] 18820556  31.368%  32.986% ######
(       3,       4 ] 19737623  32.896%  65.882% #######
(       4,       6 ] 15126207  25.210%  91.093% #####
(       6,      10 ]  1900919   3.168%  94.261% #
(      10,      15 ]  1092364   1.821%  96.082% 
(      15,      22 ]  1317136   2.195%  98.277% 
(      22,      34 ]   416732   0.695%  98.971% 
(      34,      51 ]   226569   0.378%  99.349% 
(      51,      76 ]    41698   0.069%  99.419% 
(      76,     110 ]      146   0.000%  99.419% 
(     110,     170 ]       40   0.000%  99.419% 
(     170,     250 ]       14   0.000%  99.419% 
(     250,     380 ]       12   0.000%  99.419% 
(     380,     580 ]        3   0.000%  99.419% 
(     580,     870 ]        2   0.000%  99.419% 
(     870,    1300 ]   348113   0.580%  99.999% 
(    1300,    1900 ]       46   0.000%  99.999% 
(    1900,    2900 ]      138   0.000%  99.999% 
(    2900,    4400 ]       20   0.000%  99.999% 
(    4400,    6600 ]        3   0.000%  99.999% 
(    6600,    9900 ]        6   0.000%  99.999% 
(    9900,   14000 ]        6   0.000%  99.999% 
(   14000,   22000 ]       17   0.000%  99.999% 
(   22000,   33000 ]       11   0.000%  99.999% 
(   33000,   50000 ]        7   0.000% 100.000% 
(   50000,   75000 ]       27   0.000% 100.000% 
(   75000,  110000 ]       19   0.000% 100.000% 
(  110000,  170000 ]       11   0.000% 100.000% 
(  170000,  250000 ]       16   0.000% 100.000% 
(  250000,  380000 ]       44   0.000% 100.000% 
(  380000,  570000 ]       54   0.000% 100.000% 
(  570000,  860000 ]       21   0.000% 100.000% 
(  860000, 1200000 ]        6   0.000% 100.000% 
( 1200000, 1900000 ]       14   0.000% 100.000% 
( 1900000, 2900000 ]        6   0.000% 100.000% 
( 2900000, 4300000 ]       26   0.000% 100.000% 
( 4300000, 6500000 ]       19   0.000% 100.000% 
( 6500000, 9800000 ]       14   0.000% 100.000% 
( 9800000, 14000000 ]        4   0.000% 100.000% 
( 14000000, 22000000 ]        3   0.000% 100.000% 
( 22000000, 33000000 ]        3   0.000% 100.000% 
( 33000000, 49000000 ]        3   0.000% 100.000% 
( 49000000, 74000000 ]        6   0.000% 100.000% 

DB path: [/home/supermt/rocksdb_hdd]
using reporter agent with change points.
Using FEAT tuner.
 FEA is triggered
TEA is triggered
insert start:0
record count:60000000
op count:60000000
new keys:6000000
slow flush, decrease thread
ycsb_run     :      53.731 micros/op 18611 ops/sec;
Microseconds per seek:
Count: 63649670 Average: 55.4706  StdDev: 48.63
Min: 5  Median: 53.7839  Max: 1128470
Percentiles: P50: 53.78 P75: 71.84 P99: 109.81 P99.9: 168.33 P99.99: 240.46
------------------------------------------------------
(       4,       6 ]       11   0.000%   0.000% 
(       6,      10 ]    34887   0.055%   0.055% 
(      10,      15 ]   439267   0.690%   0.745% 
(      15,      22 ]  2638234   4.145%   4.890% #
(      22,      34 ]  9988901  15.694%  20.583% ###
(      34,      51 ] 16270642  25.563%  46.146% #####
(      51,      76 ] 22027328  34.607%  80.753% #######
(      76,     110 ] 11678483  18.348%  99.101% ####
(     110,     170 ]   522850   0.821%  99.923% 
(     170,     250 ]    48482   0.076%  99.999% 
(     250,     380 ]      509   0.001% 100.000% 
(     380,     580 ]       59   0.000% 100.000% 
(     580,     870 ]        4   0.000% 100.000% 
(     870,    1300 ]        4   0.000% 100.000% 
(    1300,    1900 ]        2   0.000% 100.000% 
(    1900,    2900 ]        1   0.000% 100.000% 
(    4400,    6600 ]        1   0.000% 100.000% 
(    6600,    9900 ]        1   0.000% 100.000% 
(    9900,   14000 ]        1   0.000% 100.000% 
(   33000,   50000 ]        2   0.000% 100.000% 
(  860000, 1200000 ]        1   0.000% 100.000% 

Microseconds per write:
Count: 3350329 Average: 20.6915  StdDev: 420.54
Min: 2  Median: 9.6614  Max: 1060756
Percentiles: P50: 9.66 P75: 12.46 P99: 20.26 P99.9: 29.84 P99.99: 1512.27
------------------------------------------------------
(       1,       2 ]        7   0.000%   0.000% 
(       2,       3 ]     5595   0.167%   0.167% 
(       3,       4 ]    41102   1.227%   1.394% 
(       4,       6 ]   156287   4.665%   6.059% #
(       6,      10 ]  1608304  48.004%  54.063% ##########
(      10,      15 ]  1428033  42.624%  96.687% #########
(      15,      22 ]   103183   3.080%  99.767% #
(      22,      34 ]     6838   0.204%  99.971% 
(      34,      51 ]      300   0.009%  99.980% 
(      51,      76 ]       43   0.001%  99.981% 
(      76,     110 ]       29   0.001%  99.982% 
(     110,     170 ]       48   0.001%  99.983% 
(     170,     250 ]       55   0.002%  99.985% 
(     250,     380 ]       78   0.002%  99.987% 
(     380,     580 ]       39   0.001%  99.988% 
(     580,     870 ]       20   0.001%  99.989% 
(     870,    1300 ]       22   0.001%  99.990% 
(    1300,    1900 ]       31   0.001%  99.991% 
(    1900,    2900 ]       11   0.000%  99.991% 
(    2900,    4400 ]       10   0.000%  99.991% 
(    4400,    6600 ]        5   0.000%  99.991% 
(    6600,    9900 ]       18   0.001%  99.992% 
(    9900,   14000 ]       10   0.000%  99.992% 
(   14000,   22000 ]       28   0.001%  99.993% 
(   22000,   33000 ]       36   0.001%  99.994% 
(   33000,   50000 ]       57   0.002%  99.996% 
(   50000,   75000 ]       66   0.002%  99.998% 
(   75000,  110000 ]       37   0.001%  99.999% 
(  110000,  170000 ]        2   0.000%  99.999% 
(  170000,  250000 ]        3   0.000%  99.999% 
(  250000,  380000 ]        5   0.000%  99.999% 
(  380000,  570000 ]        4   0.000%  99.999% 
(  570000,  860000 ]       16   0.000% 100.000% 
(  860000, 1200000 ]        7   0.000% 100.000% 



** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  L0      2/0   590.66 MB   0.5      1.5     0.0      1.5      61.4     59.8       0.0   1.0      1.3     51.1   1230.40            177.01       128    9.612   1581K      0
  L1     10/0    1.97 GB   1.0    104.2    59.3     44.9     104.0     59.1       0.0   1.8    107.0    106.8    997.33            194.76        15   66.488    107M   173K
  L2     54/0   19.51 GB   1.0    115.2    50.3     64.9     115.1     50.2       6.8   2.3     31.8     31.7   3714.20            282.58       101   36.774    119M    95K
  L3     85/0   37.46 GB   0.2     13.0     6.9      6.1      12.9      6.8      30.6   1.9     46.1     46.0    288.12             26.21        12   24.010     13M    10K
 Sum    151/0   59.52 GB   0.0    233.9   116.4    117.4     293.4    176.0      37.4   4.9     38.4     48.2   6230.05            680.56       256   24.336    241M   279K
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.3      0.3       0.0   1.0      0.0    171.2      1.72              0.65         1    1.724       0      0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Low      0/0    0.00 KB   0.0    233.9   116.4    117.4     233.6    116.1       0.0   0.0     47.5     47.4   5044.93            507.17       129   39.108    241M   279K
High      0/0    0.00 KB   0.0      0.0     0.0      0.0      59.8     59.8       0.0   0.0      0.0     51.7   1185.12            173.38       127    9.332       0      0
Uptime(secs): 5326.6 total, 526.6 interval
Flush(GB): cumulative 59.840, interval 0.288
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 293.41 GB write, 56.40 MB/s write, 233.89 GB read, 44.96 MB/s read, 6230.0 seconds
Interval compaction: 0.29 GB write, 0.56 MB/s write, 0.00 GB read, 0.00 MB/s read, 1.7 seconds
Stalls(count): 9 level0_slowdown, 9 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 48 slowdown for pending_compaction_bytes, 290 memtable_compaction, 0 memtable_slowdown, interval 0 total count

** File Read Latency Histogram By Level (Include Compaction) [default] **

** File Read Latency Histogram By Level (Without Compaction) [default] **

** DB Stats **
Uptime(secs): 5326.6 total, 526.6 interval
Cumulative writes: 63M writes, 63M keys, 63M commit groups, 1.0 writes per commit group, ingest: 61.32 GB, 11.79 MB/s
Cumulative WAL: 63M writes, 0 syncs, 63350329.00 writes per sync, written: 61.32 GB, 11.79 MB/s
Cumulative stall: 00:22:20.473 H:M:S, 25.2 percent
Cumulative L0 stall: 00:03:55.563 H:M:S
Cumulative Mem stall: 00:14:5.959 H:M:S
Cumulative Pending stall: 00:04:18.952 H:M:S
Interval writes: 501K writes, 501K keys, 501K commit groups, 1.0 writes per commit group, ingest: 496.83 MB, 0.94 MB/s
Interval WAL: 501K writes, 0 syncs, 501233.00 writes per sync, written: 0.49 MB, 0.94 MB/s
Interval stall: 00:00:0.000 H:M:S, 0.0 percent
Interval L0 stall: 00:00:0.000 H:M:S
Interval Mem stall: 00:00:0.000 H:M:S
Interval Pending stall: 00:00:0.000 H:M:S

